digraph {
6 [style = filled, label = "Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\",spark.version().startsWith(\"2\"))@@@3@@@['1', '1', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB1BBB"];
16 [style = filled, label = "testNullableWithSparkSqlOption['0', '0', '1']", fillcolor = lightgray, shape = diamond image = "AAA0AAABBB3BBB"];
2 [style = filled, label = "tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION,targetPath)@@@7@@@['1', '1', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB1BBB"];
12 [style = filled, label = "testNullableWithWriteOption['0', '1', '0']", fillcolor = lightgray, shape = diamond image = "AAA0AAABBB2BBB"];
9 [style = filled, label = "new HadoopTables(spark.sessionState().newHadoopConf()).create(icebergSchema,PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),tableProperties,targetPath)@@@9@@@['1', '1', '0']", fillcolor = white, shape = ellipse image = "AAA0AAABBB1BBB"];
15 [style = filled, label = "new HadoopTables(newSparkSession.sessionState().newHadoopConf()).create(icebergSchema,PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),tableProperties,targetPath)@@@10@@@['0', '0', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB3BBB"];
1 [style = filled, label = "File location = new File(temp.newFolder(\"parquet\"),\"test\")@@@4@@@['1', '1', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB1BBB"];
20 [style = filled, label = "newSparkSession.read().schema(sparkSchema).json(JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0)).write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath)@@@11@@@['0', '0', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB3BBB"];
7 [style = filled, label = "spark.read().schema(sparkSchema).json(JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1)).write().parquet(sourcePath)@@@8@@@['1', '1', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB1BBB"];
14 [style = filled, label = "List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList()@@@13@@@['0', '0', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB3BBB"];
11 [style = filled, label = "spark.read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath).write().format(\"iceberg\").option(\"check-nullability\",false).mode(SaveMode.Append).save(targetPath)@@@11@@@['1', '0', '0']", fillcolor = red, shape = ellipse image = "AAA1AAABBB1BBB"];
10 [style = filled, label = "spark.read().schema(sparkSchema).json(JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0)).write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath)@@@10@@@['1', '1', '0']", fillcolor = white, shape = ellipse image = "AAA0AAABBB1BBB"];
5 [style = filled, label = "testNullableWithWriteOption['1', '0', '0']", fillcolor = lightgray, shape = diamond image = "AAA0AAABBB1BBB"];
19 [style = filled, label = "SparkSession newSparkSession = SparkSession.builder().master(\"local(2(\").appName(\"NullableTest\").config(\"spark.sql.iceberg.check-nullability\",false).getOrCreate()@@@9@@@['0', '0', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB3BBB"];
0 [style = filled, label = "Assert.assertEquals(\"Should contain 6 rows\",6,rows.size())@@@13@@@['1', '1', '0']", fillcolor = lightgray, shape = ellipse image = "AAA0AAABBB1BBB"];
13 [style = filled, label = "spark.read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath).write().format(\"iceberg\").option(SparkWriteOptions.CHECK_NULLABILITY,false).mode(SaveMode.Append).save(targetPath)@@@11@@@['0', '1', '0']", fillcolor = red, shape = ellipse image = "AAA1AAABBB2BBB"];
3 [style = filled, label = "String targetPath = String.format(\"%s/nullable_poc/targetFolder/\",location.toString())@@@6@@@['1', '1', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB1BBB"];
4 [style = filled, label = "List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList()@@@12@@@['1', '1', '0']", fillcolor = white, shape = ellipse image = "AAA0AAABBB1BBB"];
18 [style = filled, label = "newSparkSession.read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath).write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath)@@@12@@@['0', '0', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB3BBB"];
17 [style = filled, label = "Assert.assertEquals(\"Should contain 6 rows\",6,rows.size())@@@14@@@['0', '0', '1']", fillcolor = lightgray, shape = ellipse image = "AAA0AAABBB3BBB"];
8 [style = filled, label = "String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\",location.toString())@@@5@@@['1', '1', '1']", fillcolor = white, shape = ellipse image = "AAA0AAABBB1BBB"];
2->9 [style = solid, label="tableProperties"];
4->18 [style = dashed, label="0"];
3->9 [style = solid, label="targetPath"];
8->7 [style = solid, label="sourcePath"];
1->8 [style = bold, label=""];
11->20 [style = dashed, label="0"];
16->6 [style = bold, label=""];
11->4 [style = bold, label=""];
8->3 [style = bold, label=""];
14->17 [style = bold, label=""];
9->10 [style = bold, label=""];
3->15 [style = solid, label="targetPath"];
12->6 [style = bold, label=""];
2->15 [style = solid, label="tableProperties"];
4->0 [style = bold, label=""];
3->10 [style = solid, label="targetPath"];
7->9 [style = bold, label=""];
20->18 [style = bold, label=""];
6->1 [style = bold, label=""];
11->13 [style = dashed, label="0"];
10->11 [style = bold, label=""];
5->6 [style = bold, label=""];
3->2 [style = bold, label=""];
10->13 [style = bold, label=""];
15->20 [style = bold, label=""];
3->18 [style = solid, label="targetPath"];
3->2 [style = solid, label="targetPath"];
3->20 [style = solid, label="targetPath"];
3->13 [style = solid, label="targetPath"];
2->7 [style = bold, label=""];
19->15 [style = bold, label=""];
3->11 [style = solid, label="targetPath"];
7->19 [style = bold, label=""];
18->14 [style = bold, label=""];
13->4 [style = bold, label=""];
}
